{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Get rid of things in parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique name variants:  108212\n"
     ]
    }
   ],
   "source": [
    "names  = [line.strip() for line in open('Ikon/extracted/all_artists.dat')]\n",
    "cntr_o = Counter(names)  \n",
    "print('Unique name variants: ', len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.DataFrame([(a,b) for a, b in dict(cntr_o).items()])\n",
    "df_0 = df_0.rename(columns = {0 : 'step0', 1: 'frequency_original'})\n",
    "df_0.to_csv('Ikon/cleaning_steps/original_cnt.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names:   108203\n",
      "Unique names:  27498\n"
     ]
    }
   ],
   "source": [
    "fout = open('Ikon/cleaning_steps/step1.csv', 'w')\n",
    "\n",
    "names1 = []\n",
    "step0_step1 = []\n",
    "\n",
    "for n in names:\n",
    "        \n",
    "    c = n.replace('\\t', ' ')\n",
    "    if ('(' in n and ')' in n) or ('[' in n and ']' in n):\n",
    "        c = re.sub(r'\\([^)]*\\)', '', c)  \n",
    "        c = re.sub(r'\\[[^)]*\\]', '', c)  \n",
    "\n",
    "    if len(c) > 0:\n",
    "        names1.append(n)\n",
    "        fout.write(n + '\\t' + c + '\\n')\n",
    "        step0_step1.append((n, c))\n",
    "    else:\n",
    "        step0_step1.append((n, 'DELETED'))\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "fout.close()\n",
    "    \n",
    "print('Total names:  ', len(names1)) \n",
    "print('Unique names: ', len(set(names)))\n",
    "\n",
    "df_s12 = pd.DataFrame(step0_step1)\n",
    "df_s12 = df_s12.rename(columns = {0 : 'step0', 1: 'step1'})\n",
    "df_s12.to_csv('Ikon/cleaning_steps/step0_step1.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Splitting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names  = [line.strip().split('\\t')[1] for line in open('Ikon/cleaning_steps/step1.csv')]\n",
    "names1 = []\n",
    "sep    = ' / | &| •• | \\*\\*| \\* | \\| \\& | and | • | – | és | - '\n",
    "fout   = open('Ikon/cleaning_steps/step2.csv', 'w')\n",
    "\n",
    "step1_step2 = []\n",
    "\n",
    "\n",
    "for name in names:\n",
    "\n",
    "    name2  = name.replace('|', 'and')\n",
    "    splits = re.split(sep, name2)\n",
    "    \n",
    "    for s in splits:\n",
    "        if len(s) > 0:\n",
    "            fout.write(name + '\\t' + s + '\\n')\n",
    "            names1.append(s)\n",
    "            step1_step2.append((name, s))\n",
    "        else:\n",
    "            names1.append('DELETED')\n",
    "            step1_step2.append((name, 'DELETED ' + str(random.random())))\n",
    "\n",
    "fout.close()\n",
    "            \n",
    "len(names1), len(set(names1))\n",
    "\n",
    "\n",
    "df_s12 = pd.DataFrame(step1_step2)\n",
    "df_s12 = df_s12.rename(columns = {0 : 'step1', 1: 'step2'})\n",
    "df_s12.to_csv('Ikon/cleaning_steps/step1_step2.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Cleaning strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111634 24540 111634\n"
     ]
    }
   ],
   "source": [
    "names  = [line.strip().split('\\t')[1] for line in open('Ikon/cleaning_steps/step2.csv')]\n",
    "names1 = []\n",
    "fout   = open('Ikon/cleaning_steps/step3.csv', 'w')\n",
    "names0 = []\n",
    "\n",
    "step2_step3 = []\n",
    "\n",
    "\n",
    "to_replace = [\n",
    "    '..', ',',\n",
    "    '-,',\n",
    "    '-', ';', '.', '/', '?', '*', '!', '+', '••',\n",
    "    ';.', ':', 'Dr', 'Prof', 'művészettörténész', 'plakátművész', 'kortárs művész', 'résztvevő', 'résztvevők', \n",
    "    'képzőművész', ' művész', ' alkotó', 'filmesztéta', 'építész', 'eszmetörténész', 'országos széchényi könyvtár',\n",
    "    'az ', 'Budapest', '(', ')', '[', ']',\n",
    "    '\"']\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    name1 = name\n",
    "    names0.append(name)\n",
    "\n",
    "    for tr in to_replace:\n",
    "        name1 = name1.replace(tr, '')\n",
    "        \n",
    "    name1 = name1.strip().rstrip()\n",
    "    \n",
    "    if len(name1) > 1:\n",
    "        fout.write(name + '\\t' + name1 + '\\n')  \n",
    "        names1.append(name1)\n",
    "        step2_step3.append((name, name1))\n",
    "    else:\n",
    "        names1.append('DELETED')\n",
    "        step2_step3.append((name, 'DELETED'))\n",
    "\n",
    "        \n",
    "  \n",
    "fout.close()\n",
    "        \n",
    "print(len(names1), len(set(names1)), len(names))\n",
    "\n",
    "df_s23 = pd.DataFrame(step2_step3)\n",
    "df_s23 = df_s23.rename(columns = {0 : 'step2', 1: 'step3'})\n",
    "df_s23.to_csv('Ikon/cleaning_steps/step2_step3.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Drop numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105115 24206\n"
     ]
    }
   ],
   "source": [
    "names  = [line.strip().split('\\t')[1] for line in open('Ikon/cleaning_steps/step3.csv')]\n",
    "names1 = []\n",
    "fout   = open('Ikon/cleaning_steps/step4.csv', 'w')\n",
    "\n",
    "step3_step4 = []\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    nums = re.findall('\\d', name)\n",
    "    if len(nums) == 0:\n",
    "        fout.write(name + '\\t' + name + '\\n')  \n",
    "        names1.append(name)\n",
    "        step3_step4.append((name, name))\n",
    "    else:\n",
    "        step3_step4.append((name, 'DELETED'))\n",
    "        \n",
    "\n",
    "fout.close()\n",
    "       \n",
    "print(len(names1), len(set(names1)))\n",
    "\n",
    "df_s34 = pd.DataFrame(step3_step4)\n",
    "df_s34 = df_s34.rename(columns = {0 : 'step3', 1: 'step4'})\n",
    "df_s34.to_csv('Ikon/cleaning_steps/step3_step4.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Drop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words  = [line.strip() for line in open('web2.2-freq-sorted.top100k.nofreqs.txt' , encoding='latin2')]\n",
    "top_words += ['MKE', 'Magyar Képzőmìvészeti Egyetem', 'kiállítást', 'művészek', 'könyvet', 'bemutatja', 'művészettörténész','Évzáró party', 'Fiatal Képzőművészek Stúdiója']\n",
    "\n",
    "top_words = set(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105115\n"
     ]
    }
   ],
   "source": [
    "names  = [line.strip().split('\\t')[1] for line in open('Ikon/cleaning_steps/step4.csv')]\n",
    "names1 = []\n",
    "fout   = open('Ikon/cleaning_steps/step5.csv', 'w')\n",
    "\n",
    "step4_step5 = []\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    words   = name.split(' ')\n",
    "    profile = []\n",
    "    name2 = []\n",
    "    for w in words:\n",
    "        \n",
    "        if len(w) > 1:\n",
    "            if w[0] == w[0].lower() and w in top_words:\n",
    "                pass\n",
    "              \n",
    "            else:\n",
    "                name2.append(w)\n",
    "\n",
    "                    \n",
    "\n",
    "    #print(words, profile, name2)\n",
    "        \n",
    "    if len(name2) > 1:\n",
    "        name2 = ' '.join(name2).strip().rstrip()\n",
    "        fout.write(name + '\\t' + name2 + '\\n')  \n",
    "        names1.append(name2)\n",
    "        step4_step5.append((name, name2))\n",
    "        \n",
    "    else:\n",
    "        step4_step5.append((name, 'DELETED'))\n",
    "\n",
    "\n",
    "fout.close()\n",
    "       \n",
    "        \n",
    "len(names1), len(set(names1))\n",
    "\n",
    "\n",
    "df_s45 = pd.DataFrame(step4_step5)\n",
    "print(len(df_s45))\n",
    "df_s45 = df_s45.rename(columns = {0 : 'step4', 1: 'step5'})\n",
    "df_s45.to_csv('Ikon/cleaning_steps/step4_step5.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Drop rare and unique variants\n",
    "\n",
    " Similarity - are items & Count variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97612 18304\n"
     ]
    }
   ],
   "source": [
    "### save lower-case name variants without accents\n",
    "\n",
    "names  = [line.strip().split('\\t')[1] for line in open('Ikon/cleaning_steps/step5.csv')]\n",
    "names1 = []\n",
    "fout   = open('Ikon/cleaning_steps/step6.csv', 'w')\n",
    "\n",
    "step5_step6 = []\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    name1 = unidecode.unidecode(name).lower()\n",
    "\n",
    "    if len(name1) > 0:\n",
    "        fout.write(name + '\\t' + name1 + '\\n')  \n",
    "        names1.append(name1)\n",
    "        step5_step6.append((name, name1))\n",
    "    else:     \n",
    "        step5_step6.append((name, 'DELETED'))\n",
    "\n",
    "fout.close()    \n",
    "    \n",
    "print(len(names1), len(set(names1)))\n",
    "\n",
    "df_s56 = pd.DataFrame(step5_step6)\n",
    "df_s56 = df_s56.rename(columns = {0 : 'step5', 1: 'step6'})\n",
    "df_s56.to_csv('Ikon/cleaning_steps/step5_step6.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  frequency distriution of the variants\n",
    "cntr = dict(Counter(names1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palman zsuzsanna --- palmann zsuzsi\n",
      "palman zsuzsanna --- papp zsuzsanna\n",
      "palmann zsuzsi --- palman zsuzsanna\n",
      "pap kata --- papp katalin\n",
      "papp gábor --- papp tibor\n",
      "papp katalin --- pap kata\n",
      "papp kinga --- papp réka kinga\n",
      "papp réka kinga --- papp kinga\n",
      "papp tibor --- papp gábor\n",
      "papp zsuzsanna --- palman zsuzsanna\n",
      "part --- partum\n",
      "partum --- part\n",
      "pataki szandra --- pataki zora\n",
      "pataki zora --- pataki szandra\n",
      "paul --- paul p\n",
      "paul kessel --- paul klee\n",
      "paul klee --- paul kessel\n",
      "paul p --- paul\n",
      "paál csaba --- pléh csaba\n",
      "peer krisztián --- peer krisztián  költő\n",
      "peer krisztián  költő --- peer krisztián\n",
      "performance --- performer\n",
      "performer --- performance\n",
      "perényi tamás --- petrik tamás\n",
      "peternák miklós --- petrányi miklós\n",
      "petrik tamás --- perényi tamás\n",
      "petrika --- petruska\n",
      "petrina ildikó --- pető anna ildikó\n",
      "petruska --- petrika\n",
      "petrányi miklós --- peternák miklós\n",
      "petöcz andrás --- petőcz andrás író\n",
      "pető anna ildikó --- petrina ildikó\n",
      "petőcz andrás költő --- petőcz andrás mv\n",
      "petőcz andrás mv --- petőcz andrás költő\n",
      "petőcz andrás író --- petöcz andrás\n",
      "pggroup --- pp group\n",
      "pintér dia --- pintér éva\n",
      "pintér georg --- pintér györgy\n",
      "pintér gergő --- pintér györgy\n",
      "pintér györgy --- pintér georg\n",
      "pintér györgy --- pintér gergő\n",
      "pintér éva --- pintér dia\n",
      "pion istván --- polónyi istván\n",
      "pléh csaba --- paál csaba\n",
      "pogátsa zoltán --- poós zoltán\n",
      "polónyi istván --- pion istván\n",
      "pozsár eszter --- pozsár péter\n",
      "pozsár péter --- pozsár eszter\n",
      "poós zoltán --- pogátsa zoltán\n",
      "pp group --- pggroup\n"
     ]
    }
   ],
   "source": [
    "###  check on the similarity values --- 80 seems like a reasonable threshold\n",
    "\n",
    "for fn in os.listdir('Ikon/name_matching')[15:16]:  \n",
    "    with open('Ikon/name_matching/' + fn) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            n1, n2, score = line.strip().split('\\t')\n",
    "            score = float(score)\n",
    "            if score <= 80 and score > 79:\n",
    "                print(n1, '---', n2)\n",
    "                \n",
    "                \n",
    "### --> 80 similarity is different enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6292 11857\n",
      "18149\n"
     ]
    }
   ],
   "source": [
    "###  drop unique and rare variants\n",
    "\n",
    "step7_stats = []\n",
    "\n",
    "alll   = []\n",
    "drop   = []\n",
    "unique = []\n",
    "for line in open('Ikon/similarities/COMBINED_scores.dat'):\n",
    "    name, scores = line.strip().split('\\t\\t')\n",
    "    scores = [int(s) for s in scores.split('\\t')]\n",
    "    \n",
    "    \n",
    "\n",
    "    if name in cntr:\n",
    "        step7_stats.append((name, max(scores), cntr[name]))\n",
    "        alll.append(name)\n",
    "        if max(scores) < 80 and cntr[name] < 3:\n",
    "            drop.append(name)\n",
    "        else:\n",
    "            unique.append(name)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print(len(unique), len(drop))\n",
    "\n",
    "df_s7 = pd.DataFrame(step7_stats)\n",
    "print(len(df_s7))\n",
    "df_s7 = df_s7.rename(columns = {0 : 'step6', 1 : 'version 6 - top sim', 2 : 'version 6 - cnt'})\n",
    "df_s7.to_csv('Ikon/cleaning_steps/step7_stats.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Similarity graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97612"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### counter\n",
    "cntr = dict(Counter(names1))\n",
    "len(names1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the similarity values between all pairs of name variants\n",
    "pairs_similarities = []\n",
    "nodes_all          = set()\n",
    "\n",
    "files = os.listdir('Ikon/name_matching')\n",
    "sims  = []\n",
    "\n",
    "for fn in files:  \n",
    "    with open('Ikon/name_matching/' + fn) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            n1, n2, score = line.strip().split('\\t')\n",
    "            pairs_similarities.append((n1, n2, float(score)))  \n",
    "            sims.append(float(score))\n",
    "            nodes_all.add(n1)\n",
    "            nodes_all.add(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5900, 4410)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### build the similarity w a given threshold\n",
    "G = nx.Graph()\n",
    "\n",
    "similarity_limit = 79\n",
    "\n",
    "for n1, n2, s in pairs_similarities:\n",
    "    if s > similarity_limit:\n",
    "        G.add_edge(n1, n2, weight = s)\n",
    "        \n",
    "len(G.nodes), len(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16719\n",
      "22619\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_component_id</th>\n",
       "      <th>step6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>szabó ákos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>szabó netta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>szabics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>szabó iván</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>szabó tamara</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   similarity_component_id         step6\n",
       "0                        0    szabó ákos\n",
       "1                        0   szabó netta\n",
       "2                        0       szabics\n",
       "3                        0    szabó iván\n",
       "4                        0  szabó tamara"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### hasonlosagi komponensek listazasa\n",
    "\n",
    "nodes_added    = set()\n",
    "components     = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "names_clusters = []\n",
    "\n",
    "for ind, c in enumerate(components):  \n",
    "\n",
    "    for cc in list(c):\n",
    "        names_clusters.append((ind, cc))\n",
    "        nodes_added.add(cc)\n",
    "\n",
    "\n",
    "nodes_missed = list(nodes_all.difference(nodes_added))\n",
    "\n",
    "print(len(nodes_missed))\n",
    "\n",
    "for jnd, nm in enumerate(nodes_missed):\n",
    "    names_clusters.append((jnd+ind, nm))\n",
    "        \n",
    "\n",
    "df_comp = pd.DataFrame(names_clusters)\n",
    "df_comp = df_comp.rename(columns = {1 : 'step6', 0: 'similarity_component_id'})\n",
    "df_comp.to_csv('Ikon/cleaning_steps/components.csv', sep = '\\t')\n",
    "print(len(df_comp))\n",
    "df_comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step x --- output:\n",
    "- grafkomponens szerint csoprtositva\n",
    "- name variant cnt, verzio, index oszlopok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
